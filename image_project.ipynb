{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Wednesday December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to a set of images.  The images are originally from a photo-sharing site and released under Creative Commons-licenses allowing sharing.  The training set contains 20 000 images. We have resized them and cropped them to 128x128 to make the task a bit more manageable.\n",
    "\n",
    "We're only giving you the code for downloading the data. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the image project:\n",
    "\n",
    "- One image may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are images that don't belong to any of our classes, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "\n",
    "- As the dataset is pretty imbalanced, don't focus too strictly on the outputs being probabilistic. (Meaning that the right threshold for selecting the label might not be 0.5.)\n",
    "\n",
    "- Image files can be loaded as numpy matrices for example using `imageio.imread`. Most images are color, but a few grayscale. You need to handle the grayscale ones somehow as they would have a different number of color channels (depth) than the color ones.\n",
    "\n",
    "- Loading all the images into one big matrix as we have done in the exercises is not feasible (e.g. the virtual servers in CSC have only 3 GB of RAM). You need to load the images in smaller chunks for the training. This shouldn't be a problem we are doing mini-batch training anyway, and thus we don't need to keep all the images in memory. You can simply pass you current chunk of images to `model.fit()` as it remembers the weights from the previous run.\n",
    "\n",
    "- You need to think carefully about how you load the annotations and match them up with the corresponding images, especially as you are loading them in smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./train/dl2017-image-proj.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "database_path = 'train/'\n",
    "\n",
    "dl_file='dl2017-image-proj.zip'\n",
    "dl_url='https://www.cs.helsinki.fi/u/mvsjober/misc/'\n",
    "get_file(dl_file, dl_url+dl_file, cache_dir='./', cache_subdir=database_path, extract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloaded and extracted the data files into the `train` subdirectory.\n",
    "\n",
    "The images can be found in `train/images`, and are named as `im1.jpg`, `im2.jpg` and so on until `im20000.jpg`.\n",
    "\n",
    "The class labels, or annotations, can be found in `train/annotations` as `CLASSNAME.txt`, where CLASSNAME is one of the fourteen classes: *baby, bird, car, clouds, dog, female, flower, male, night, people, portrait, river, sea,* and *tree*.\n",
    "\n",
    "Each annotation file is a simple text file that lists the images that depict that class, one per line. The images are listed with their number, not the full filename. For example `5969` refers to the image `im5969.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import misc\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = {0:\"baby\",\n",
    "                 1:\"bird\",\n",
    "                 2:\"car\",\n",
    "                 3:\"clouds\",\n",
    "                 4:\"dog\",\n",
    "                 5:\"female\",\n",
    "                 6:\"flower\",\n",
    "                 7:\"male\",\n",
    "                 8:\"night\",\n",
    "                 9:\"people\",\n",
    "                 10:\"portrait\",\n",
    "                 11:\"river\",\n",
    "                 12:\"sea\",\n",
    "                 13:\"tree\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label and file name loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20000\n",
    "num_class = len(class_indices)\n",
    "\n",
    "\n",
    "img_list = [database_path+\"images/\"+\"im\"+str(x)+\".jpg\" for x in range(1,N+1)]\n",
    "y = np.zeros((N,num_class))\n",
    "\n",
    "for k,v in class_indices.items():\n",
    "    with open(database_path+\"annotations/\"+v+\".txt\",\"r\") as fp:\n",
    "        for line in fp:\n",
    "            y[int(line)-1,k] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (32,32)\n",
    "\n",
    "image_load_func = lambda path: misc.imresize(misc.imread(path,mode=\"RGB\"),size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "people\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABrCAYAAABnlHmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe110ab1710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example=3\n",
    "img = image_load_func(img_list[example])\n",
    "cmaps = [plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]\n",
    "for l in range(3):\n",
    "    plt.subplot(1, 4, l+1)\n",
    "    plt.imshow(img[:,:,l],cmap=cmaps[l])\n",
    "    plt.axis('off')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "if sum(y[example]==1)==0:\n",
    "    print(\"No label\")\n",
    "else:\n",
    "    for i in np.argwhere(y[example]==1).flatten():\n",
    "        print(class_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack([image_load_func(file) for file in img_list])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from densenet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThreshold(y_pred,y_true,threshold_interval):\n",
    "    threshold = np.arange(0,1,threshold_interval)\n",
    "\n",
    "    best_threshold = np.zeros(y_true.shape[1])\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        temp = np.array([[1 if pred>j else 0 for j in threshold] for pred in y_pred[:,i]])\n",
    "        score = np.array([f1_score(y_true[:,i],temp[:,j], average='micro') for j in range(len(threshold)) ])\n",
    "        best_threshold[i] = threshold[score.argmax()]\n",
    "        print(\"Best threshold for class\",i,\"ï¼š\",best_threshold[i])\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple DenseNet with minor adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-parameter\n",
    "nb_filter=32\n",
    "nb_dense_block=1\n",
    "nb_layers=3\n",
    "growth_rate=24\n",
    "dropout_rate=0.3\n",
    "weight_decay=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv2D)          (None, 32, 32, 32)    864         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 32, 32, 32)    128         initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32, 32, 32)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 32, 24)    6936        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 32, 32, 24)    0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 32, 32, 56)    0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 32, 32, 56)    128         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32, 32, 56)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 32, 32, 24)    12120       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 32, 32, 24)    0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 32, 32, 80)    0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 32, 32, 80)    128         concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32, 32, 80)    0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 32, 32, 24)    17304       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32, 32, 24)    0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 32, 32, 104)   0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "                                                                   dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 32, 32, 104)   128         concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 32, 32, 104)   0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 16, 16, 104)   0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 26624)         0           average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           13632000    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 512)           2048        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 512)           0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 512)           0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 14)            7182        dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 13,678,966\n",
      "Trainable params: 13,677,686\n",
      "Non-trainable params: 1,280\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "dn = DenseNet(num_class, X.shape[1:], nb_layers*3+4, nb_dense_block, growth_rate,\n",
    "             nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "dn.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam')\n",
    "\n",
    "print(dn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1300pt\" viewBox=\"0.00 0.00 734.00 1300.00\" width=\"734pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1296)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1296 730,-1296 730,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140472932884944 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140472932884944</title>\n",
       "<polygon fill=\"none\" points=\"344,-1245.5 344,-1291.5 709,-1291.5 709,-1245.5 344,-1245.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424\" y=\"-1264.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"504,-1245.5 504,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"538\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"504,-1268.5 572,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"538\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"572,-1245.5 572,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"640.5\" y=\"-1276.3\">(None, 32, 32, 3)</text>\n",
       "<polyline fill=\"none\" points=\"572,-1268.5 709,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"640.5\" y=\"-1253.3\">(None, 32, 32, 3)</text>\n",
       "</g>\n",
       "<!-- 140468365381248 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140468365381248</title>\n",
       "<polygon fill=\"none\" points=\"327,-1162.5 327,-1208.5 726,-1208.5 726,-1162.5 327,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"419.5\" y=\"-1181.8\">initial_conv2D: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"512,-1162.5 512,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"546\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"512,-1185.5 580,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"546\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"580,-1162.5 580,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"653\" y=\"-1193.3\">(None, 32, 32, 3)</text>\n",
       "<polyline fill=\"none\" points=\"580,-1185.5 726,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"653\" y=\"-1170.3\">(None, 32, 32, 32)</text>\n",
       "</g>\n",
       "<!-- 140472932884944&#45;&gt;140468365381248 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140472932884944-&gt;140468365381248</title>\n",
       "<path d=\"M526.5,-1245.37C526.5,-1237.15 526.5,-1227.66 526.5,-1218.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"530,-1218.61 526.5,-1208.61 523,-1218.61 530,-1218.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472192653072 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140472192653072</title>\n",
       "<polygon fill=\"none\" points=\"98.5,-1079.5 98.5,-1125.5 644.5,-1125.5 644.5,-1079.5 98.5,-1079.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.5\" y=\"-1098.8\">batch_normalization_1: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"430.5,-1079.5 430.5,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464.5\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"430.5,-1102.5 498.5,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464.5\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"498.5,-1079.5 498.5,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"571.5\" y=\"-1110.3\">(None, 32, 32, 32)</text>\n",
       "<polyline fill=\"none\" points=\"498.5,-1102.5 644.5,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"571.5\" y=\"-1087.3\">(None, 32, 32, 32)</text>\n",
       "</g>\n",
       "<!-- 140468365381248&#45;&gt;140472192653072 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140468365381248-&gt;140472192653072</title>\n",
       "<path d=\"M484.146,-1162.37C465.18,-1152.46 442.651,-1140.68 422.743,-1130.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"424.287,-1127.14 413.803,-1125.61 421.045,-1133.34 424.287,-1127.14\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472146335952 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140472146335952</title>\n",
       "<polygon fill=\"none\" points=\"69,-747.5 69,-793.5 654,-793.5 654,-747.5 69,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.5\" y=\"-766.8\">concatenate_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"290,-747.5 290,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"290,-770.5 358,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"358,-747.5 358,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506\" y=\"-778.3\">[(None, 32, 32, 32), (None, 32, 32, 12)]</text>\n",
       "<polyline fill=\"none\" points=\"358,-770.5 654,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506\" y=\"-755.3\">(None, 32, 32, 44)</text>\n",
       "</g>\n",
       "<!-- 140468365381248&#45;&gt;140472146335952 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140468365381248-&gt;140472146335952</title>\n",
       "<path d=\"M606.995,-1162.41C624.896,-1153.84 641.96,-1142.07 653.5,-1126 724.284,-1027.43 745.595,-958.161 682.5,-854.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M682.5,-852.5C660.113,-827.783 592.064,-808.831 524.35,-795.496\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"524.85,-792.027 514.367,-793.569 523.523,-798.901 524.85,-792.027\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472196143424 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140472196143424</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 723,-378.5 723,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.5\" y=\"-351.8\">concatenate_2: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"221,-332.5 221,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"221,-355.5 289,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"289,-332.5 289,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506\" y=\"-363.3\">[(None, 32, 32, 32), (None, 32, 32, 12), (None, 32, 32, 12)]</text>\n",
       "<polyline fill=\"none\" points=\"289,-355.5 723,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506\" y=\"-340.3\">(None, 32, 32, 56)</text>\n",
       "</g>\n",
       "<!-- 140468365381248&#45;&gt;140472196143424 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140468365381248-&gt;140472196143424</title>\n",
       "<path d=\"M682.5,-852.5C658.034,-825.488 682.5,-724.944 682.5,-688.5 682.5,-688.5 682.5,-688.5 682.5,-520.5 682.5,-443.832 608.986,-402.985 532.319,-381.233\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"533.039,-377.801 522.47,-378.545 531.196,-384.554 533.039,-377.801\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472192652064 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140472192652064</title>\n",
       "<polygon fill=\"none\" points=\"168,-996.5 168,-1042.5 569,-1042.5 569,-996.5 168,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261.5\" y=\"-1015.8\">activation_1: Activation</text>\n",
       "<polyline fill=\"none\" points=\"355,-996.5 355,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"389\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"355,-1019.5 423,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"389\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"423,-996.5 423,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"496\" y=\"-1027.3\">(None, 32, 32, 32)</text>\n",
       "<polyline fill=\"none\" points=\"423,-1019.5 569,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"496\" y=\"-1004.3\">(None, 32, 32, 32)</text>\n",
       "</g>\n",
       "<!-- 140472192653072&#45;&gt;140472192652064 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140472192653072-&gt;140472192652064</title>\n",
       "<path d=\"M370.68,-1079.37C370.376,-1071.15 370.024,-1061.66 369.694,-1052.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"373.187,-1052.47 369.319,-1042.61 366.191,-1052.73 373.187,-1052.47\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472185293736 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140472185293736</title>\n",
       "<polygon fill=\"none\" points=\"181,-913.5 181,-959.5 546,-959.5 546,-913.5 181,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-932.8\">conv2d_1: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"332,-913.5 332,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"332,-936.5 400,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"400,-913.5 400,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"473\" y=\"-944.3\">(None, 32, 32, 32)</text>\n",
       "<polyline fill=\"none\" points=\"400,-936.5 546,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"473\" y=\"-921.3\">(None, 32, 32, 12)</text>\n",
       "</g>\n",
       "<!-- 140472192652064&#45;&gt;140472185293736 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140472192652064-&gt;140472185293736</title>\n",
       "<path d=\"M367.134,-996.366C366.627,-988.152 366.041,-978.658 365.489,-969.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"368.974,-969.372 364.865,-959.607 361.987,-969.804 368.974,-969.372\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472932898240 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140472932898240</title>\n",
       "<polygon fill=\"none\" points=\"176,-830.5 176,-876.5 547,-876.5 547,-830.5 176,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-849.8\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"333,-830.5 333,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"367\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"333,-853.5 401,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"367\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"401,-830.5 401,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"474\" y=\"-861.3\">(None, 32, 32, 12)</text>\n",
       "<polyline fill=\"none\" points=\"401,-853.5 547,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"474\" y=\"-838.3\">(None, 32, 32, 12)</text>\n",
       "</g>\n",
       "<!-- 140472185293736&#45;&gt;140472932898240 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140472185293736-&gt;140472932898240</title>\n",
       "<path d=\"M362.953,-913.366C362.751,-905.152 362.516,-895.658 362.296,-886.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365.792,-886.517 362.046,-876.607 358.794,-886.69 365.792,-886.517\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472932898240&#45;&gt;140472146335952 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140472932898240-&gt;140472146335952</title>\n",
       "<path d=\"M361.5,-830.366C361.5,-822.152 361.5,-812.658 361.5,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-803.607 361.5,-793.607 358,-803.607 365,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472932898240&#45;&gt;140472196143424 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>140472932898240-&gt;140472196143424</title>\n",
       "<path d=\"M175.753,-833.822C124.233,-824.966 77.4598,-812.198 59.5,-794 26.0335,-760.09 40.5,-736.143 40.5,-688.5 40.5,-688.5 40.5,-688.5 40.5,-520.5 40.5,-443.832 114.014,-402.985 190.681,-381.233\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"191.804,-384.554 200.53,-378.545 189.961,-377.801 191.804,-384.554\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140468493618032 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140468493618032</title>\n",
       "<polygon fill=\"none\" points=\"88.5,-664.5 88.5,-710.5 634.5,-710.5 634.5,-664.5 88.5,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-683.8\">batch_normalization_2: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"420.5,-664.5 420.5,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"420.5,-687.5 488.5,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"488.5,-664.5 488.5,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-695.3\">(None, 32, 32, 44)</text>\n",
       "<polyline fill=\"none\" points=\"488.5,-687.5 634.5,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-672.3\">(None, 32, 32, 44)</text>\n",
       "</g>\n",
       "<!-- 140472146335952&#45;&gt;140468493618032 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140472146335952-&gt;140468493618032</title>\n",
       "<path d=\"M361.5,-747.366C361.5,-739.152 361.5,-729.658 361.5,-720.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-720.607 361.5,-710.607 358,-720.607 365,-720.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140468493531176 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140468493531176</title>\n",
       "<polygon fill=\"none\" points=\"161,-581.5 161,-627.5 562,-627.5 562,-581.5 161,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-600.8\">activation_2: Activation</text>\n",
       "<polyline fill=\"none\" points=\"348,-581.5 348,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"348,-604.5 416,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"416,-581.5 416,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"489\" y=\"-612.3\">(None, 32, 32, 44)</text>\n",
       "<polyline fill=\"none\" points=\"416,-604.5 562,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"489\" y=\"-589.3\">(None, 32, 32, 44)</text>\n",
       "</g>\n",
       "<!-- 140468493618032&#45;&gt;140468493531176 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140468493618032-&gt;140468493531176</title>\n",
       "<path d=\"M361.5,-664.366C361.5,-656.152 361.5,-646.658 361.5,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-637.607 361.5,-627.607 358,-637.607 365,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472846612408 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140472846612408</title>\n",
       "<polygon fill=\"none\" points=\"179,-498.5 179,-544.5 544,-544.5 544,-498.5 179,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-517.8\">conv2d_2: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"330,-498.5 330,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"364\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"330,-521.5 398,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"364\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"398,-498.5 398,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"471\" y=\"-529.3\">(None, 32, 32, 44)</text>\n",
       "<polyline fill=\"none\" points=\"398,-521.5 544,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"471\" y=\"-506.3\">(None, 32, 32, 12)</text>\n",
       "</g>\n",
       "<!-- 140468493531176&#45;&gt;140472846612408 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140468493531176-&gt;140472846612408</title>\n",
       "<path d=\"M361.5,-581.366C361.5,-573.152 361.5,-563.658 361.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-554.607 361.5,-544.607 358,-554.607 365,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140468493689968 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140468493689968</title>\n",
       "<polygon fill=\"none\" points=\"176,-415.5 176,-461.5 547,-461.5 547,-415.5 176,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-434.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"333,-415.5 333,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"367\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"333,-438.5 401,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"367\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"401,-415.5 401,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"474\" y=\"-446.3\">(None, 32, 32, 12)</text>\n",
       "<polyline fill=\"none\" points=\"401,-438.5 547,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"474\" y=\"-423.3\">(None, 32, 32, 12)</text>\n",
       "</g>\n",
       "<!-- 140472846612408&#45;&gt;140468493689968 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140472846612408-&gt;140468493689968</title>\n",
       "<path d=\"M361.5,-498.366C361.5,-490.152 361.5,-480.658 361.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-471.607 361.5,-461.607 358,-471.607 365,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140468493689968&#45;&gt;140472196143424 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>140468493689968-&gt;140472196143424</title>\n",
       "<path d=\"M361.5,-415.366C361.5,-407.152 361.5,-397.658 361.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-388.607 361.5,-378.607 358,-388.607 365,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140471878280640 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140471878280640</title>\n",
       "<polygon fill=\"none\" points=\"88.5,-249.5 88.5,-295.5 634.5,-295.5 634.5,-249.5 88.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-268.8\">batch_normalization_3: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"420.5,-249.5 420.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"420.5,-272.5 488.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"488.5,-249.5 488.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-280.3\">(None, 32, 32, 56)</text>\n",
       "<polyline fill=\"none\" points=\"488.5,-272.5 634.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-257.3\">(None, 32, 32, 56)</text>\n",
       "</g>\n",
       "<!-- 140472196143424&#45;&gt;140471878280640 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>140472196143424-&gt;140471878280640</title>\n",
       "<path d=\"M361.5,-332.366C361.5,-324.152 361.5,-314.658 361.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-305.607 361.5,-295.607 358,-305.607 365,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472180092600 -->\n",
       "<g class=\"node\" id=\"node14\"><title>140472180092600</title>\n",
       "<polygon fill=\"none\" points=\"161,-166.5 161,-212.5 562,-212.5 562,-166.5 161,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-185.8\">activation_3: Activation</text>\n",
       "<polyline fill=\"none\" points=\"348,-166.5 348,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"348,-189.5 416,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"416,-166.5 416,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"489\" y=\"-197.3\">(None, 32, 32, 56)</text>\n",
       "<polyline fill=\"none\" points=\"416,-189.5 562,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"489\" y=\"-174.3\">(None, 32, 32, 56)</text>\n",
       "</g>\n",
       "<!-- 140471878280640&#45;&gt;140472180092600 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>140471878280640-&gt;140472180092600</title>\n",
       "<path d=\"M361.5,-249.366C361.5,-241.152 361.5,-231.658 361.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-222.607 361.5,-212.607 358,-222.607 365,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472196836488 -->\n",
       "<g class=\"node\" id=\"node15\"><title>140472196836488</title>\n",
       "<polygon fill=\"none\" points=\"52,-83.5 52,-129.5 671,-129.5 671,-83.5 52,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-102.8\">global_average_pooling2d_1: GlobalAveragePooling2D</text>\n",
       "<polyline fill=\"none\" points=\"457,-83.5 457,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"491\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"457,-106.5 525,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"491\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"525,-83.5 525,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"598\" y=\"-114.3\">(None, 32, 32, 56)</text>\n",
       "<polyline fill=\"none\" points=\"525,-106.5 671,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"598\" y=\"-91.3\">(None, 56)</text>\n",
       "</g>\n",
       "<!-- 140472180092600&#45;&gt;140472196836488 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>140472180092600-&gt;140472196836488</title>\n",
       "<path d=\"M361.5,-166.366C361.5,-158.152 361.5,-148.658 361.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-139.607 361.5,-129.607 358,-139.607 365,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140472194967536 -->\n",
       "<g class=\"node\" id=\"node16\"><title>140472194967536</title>\n",
       "<polygon fill=\"none\" points=\"217.5,-0.5 217.5,-46.5 505.5,-46.5 505.5,-0.5 217.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.5\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"345.5,-0.5 345.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"379.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"345.5,-23.5 413.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"379.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"413.5,-0.5 413.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459.5\" y=\"-31.3\">(None, 56)</text>\n",
       "<polyline fill=\"none\" points=\"413.5,-23.5 505.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459.5\" y=\"-8.3\">(None, 14)</text>\n",
       "</g>\n",
       "<!-- 140472196836488&#45;&gt;140472194967536 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>140472196836488-&gt;140472194967536</title>\n",
       "<path d=\"M361.5,-83.3664C361.5,-75.1516 361.5,-65.6579 361.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"365,-56.6068 361.5,-46.6068 358,-56.6069 365,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(dn, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = dn.fit(X,y,epochs=epochs,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 438s - loss: 0.2339   \n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 551s - loss: 0.1897   \n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 431s - loss: 0.1759   \n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 443s - loss: 0.1637   \n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 434s - loss: 0.1523   \n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 434s - loss: 0.1415   \n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 435s - loss: 0.1296   \n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 435s - loss: 0.1164   \n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 438s - loss: 0.1056   \n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 435s - loss: 0.0936   \n",
      "16000/16000 [==============================] - 237s   \n",
      "Best threshold for class 0 ï¼š 0.045\n",
      "Best threshold for class 1 ï¼š 0.095\n",
      "Best threshold for class 2 ï¼š 0.099\n",
      "Best threshold for class 3 ï¼š 0.351\n",
      "Best threshold for class 4 ï¼š 0.11\n",
      "Best threshold for class 5 ï¼š 0.283\n",
      "Best threshold for class 6 ï¼š 0.087\n",
      "Best threshold for class 7 ï¼š 0.382\n",
      "Best threshold for class 8 ï¼š 0.459\n",
      "Best threshold for class 9 ï¼š 0.463\n",
      "Best threshold for class 10 ï¼š 0.39\n",
      "Best threshold for class 11 ï¼š 0.067\n",
      "Best threshold for class 12 ï¼š 0.155\n",
      "Best threshold for class 13 ï¼š 0.149\n",
      "4000/4000 [==============================] - 59s    \n",
      "F1 Score: 0.360873969035\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 434s - loss: 0.2342   \n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 476s - loss: 0.1878   \n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 436s - loss: 0.1731   \n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 442s - loss: 0.1614   \n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 427s - loss: 0.1482   \n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 434s - loss: 0.1351   \n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 435s - loss: 0.1215   \n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 457s - loss: 0.1073   \n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 443s - loss: 0.0942   \n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 450s - loss: 0.0820   \n",
      "16000/16000 [==============================] - 269s   \n",
      "Best threshold for class 0 ï¼š 0.029\n",
      "Best threshold for class 1 ï¼š 0.146\n",
      "Best threshold for class 2 ï¼š 0.068\n",
      "Best threshold for class 3 ï¼š 0.272\n",
      "Best threshold for class 4 ï¼š 0.07\n",
      "Best threshold for class 5 ï¼š 0.552\n",
      "Best threshold for class 6 ï¼š 0.094\n",
      "Best threshold for class 7 ï¼š 0.161\n",
      "Best threshold for class 8 ï¼š 0.181\n",
      "Best threshold for class 9 ï¼š 0.441\n",
      "Best threshold for class 10 ï¼š 0.305\n",
      "Best threshold for class 11 ï¼š 0.066\n",
      "Best threshold for class 12 ï¼š 0.057\n",
      "Best threshold for class 13 ï¼š 0.116\n",
      "4000/4000 [==============================] - 99s    \n",
      "F1 Score: 0.361252932248\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 819s - loss: 0.2383   \n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 974s - loss: 0.1886   \n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 1263s - loss: 0.1728  \n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 1428s - loss: 0.1602  \n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 995s - loss: 0.1473   \n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 1014s - loss: 0.1344  \n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 1016s - loss: 0.1198  \n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 945s - loss: 0.1056   \n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 702s - loss: 0.0914   \n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 915s - loss: 0.0797   \n",
      "16000/16000 [==============================] - 583s   \n",
      "Best threshold for class 0 ï¼š 0.021\n",
      "Best threshold for class 1 ï¼š 0.058\n",
      "Best threshold for class 2 ï¼š 0.075\n",
      "Best threshold for class 3 ï¼š 0.109\n",
      "Best threshold for class 4 ï¼š 0.05\n",
      "Best threshold for class 5 ï¼š 0.357\n",
      "Best threshold for class 6 ï¼š 0.016\n",
      "Best threshold for class 7 ï¼š 0.34\n",
      "Best threshold for class 8 ï¼š 0.234\n",
      "Best threshold for class 9 ï¼š 0.579\n",
      "Best threshold for class 10 ï¼š 0.407\n",
      "Best threshold for class 11 ï¼š 0.05\n",
      "Best threshold for class 12 ï¼š 0.053\n",
      "Best threshold for class 13 ï¼š 0.149\n",
      "4000/4000 [==============================] - 163s   \n",
      "F1 Score: 0.364320880394\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 889s - loss: 0.2393   \n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 1048s - loss: 0.1922  \n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 722s - loss: 0.1773   \n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 704s - loss: 0.1666   \n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 704s - loss: 0.1559   \n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 563s - loss: 0.1463   \n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 433s - loss: 0.1357   \n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 425s - loss: 0.1236   \n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 423s - loss: 0.1114   \n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 426s - loss: 0.1015   \n",
      "16000/16000 [==============================] - 227s   \n",
      "Best threshold for class 0 ï¼š 0.033\n",
      "Best threshold for class 1 ï¼š 0.159\n",
      "Best threshold for class 2 ï¼š 0.186\n",
      "Best threshold for class 3 ï¼š 0.339\n",
      "Best threshold for class 4 ï¼š 0.052\n",
      "Best threshold for class 5 ï¼š 0.213\n",
      "Best threshold for class 6 ï¼š 0.276\n",
      "Best threshold for class 7 ï¼š 0.286\n",
      "Best threshold for class 8 ï¼š 0.486\n",
      "Best threshold for class 9 ï¼š 0.393\n",
      "Best threshold for class 10 ï¼š 0.135\n",
      "Best threshold for class 11 ï¼š 0.142\n",
      "Best threshold for class 12 ï¼š 0.056\n",
      "Best threshold for class 13 ï¼š 0.219\n",
      "4000/4000 [==============================] - 58s    \n",
      "F1 Score: 0.359817090597\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 423s - loss: 0.2355   \n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 417s - loss: 0.1908   \n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 454s - loss: 0.1760   \n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 504s - loss: 0.1640   \n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 428s - loss: 0.1514   \n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 429s - loss: 0.1400   \n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 417s - loss: 0.1272   \n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 428s - loss: 0.1146   \n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 423s - loss: 0.1025   \n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 425s - loss: 0.0899   \n",
      "16000/16000 [==============================] - 231s   \n",
      "Best threshold for class 0 ï¼š 0.04\n",
      "Best threshold for class 1 ï¼š 0.104\n",
      "Best threshold for class 2 ï¼š 0.025\n",
      "Best threshold for class 3 ï¼š 0.162\n",
      "Best threshold for class 4 ï¼š 0.05\n",
      "Best threshold for class 5 ï¼š 0.349\n",
      "Best threshold for class 6 ï¼š 0.141\n",
      "Best threshold for class 7 ï¼š 0.335\n",
      "Best threshold for class 8 ï¼š 0.332\n",
      "Best threshold for class 9 ï¼š 0.442\n",
      "Best threshold for class 10 ï¼š 0.586\n",
      "Best threshold for class 11 ï¼š 0.148\n",
      "Best threshold for class 12 ï¼š 0.04\n",
      "Best threshold for class 13 ï¼š 0.231\n",
      "4000/4000 [==============================] - 57s    \n",
      "F1 Score: 0.371342942772\n",
      "CPU times: user 1d 22h 22min, sys: 1h 42min 2s, total: 2d 4min 2s\n",
      "Wall time: 9h 33min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 10\n",
    "#5-fold cross validation\n",
    "score = []\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train,val in kf.split(X, y):\n",
    "    K.clear_session()\n",
    "    dn = DenseNet(num_class, X.shape[1:], nb_layers*3+4, nb_dense_block, growth_rate,\n",
    "             nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "    dn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    dn.fit(X[train],y[train],epochs=epochs,batch_size=64,verbose=1)\n",
    "    y_train_pred = dn.predict(X[train],batch_size=32, verbose=1)\n",
    "    threshold=getThreshold(y_train_pred,y[train],0.001)\n",
    "    y_val_pred = 1*(dn.predict(X[val],batch_size=32, verbose=1)>threshold)    \n",
    "    score.append(f1_score(y[val], y_val_pred, average='micro'))\n",
    "    print(\"F1 Score:\",score[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.363521563009 +/- 0.00418789791906\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score:\", np.mean(score),\"+/-\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCutmoizedDenseNet():\n",
    "    nb_filter=32\n",
    "    weight_decay=0.00001\n",
    "    dropout_rate=0.2\n",
    "    nb_dense_block=3\n",
    "    nb_layers=2\n",
    "    growth_rate=1\n",
    "    \n",
    "    K.clear_session()\n",
    "    model_input = Input(shape=X.shape[1:])\n",
    "\n",
    "    x = Conv2D(nb_filter, (3, 3),\n",
    "                   kernel_initializer=\"he_uniform\",\n",
    "                   padding=\"same\",\n",
    "                   name=\"initial_conv2D\",\n",
    "                   use_bias=True,\n",
    "                   kernel_regularizer=l2(weight_decay))(model_input)\n",
    "    #blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = denseblock(x, nb_layers, nb_filter, growth_rate,\n",
    "                                  dropout_rate=dropout_rate)\n",
    "        # add transition\n",
    "        x = transition(x, nb_filter, dropout_rate=dropout_rate,\n",
    "                       weight_decay=weight_decay)\n",
    "\n",
    "    #last block \n",
    "    x, nb_filter = denseblock(x, nb_layers, nb_filter, growth_rate,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  weight_decay=weight_decay)\n",
    "\n",
    "    x = BatchNormalization(axis=1,\n",
    "                           gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D(data_format=K.image_data_format())(x)\n",
    "    #more layers\n",
    "\n",
    "    model_output = Dense(num_class, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[model_input], outputs=[model_output], name=\"DenseNet\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdn=getCutmoizedDenseNet()\n",
    "cdn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "print(cdn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(cdn, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = cdn.fit(X,y,epochs=epochs,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "#5-fold cross validation\n",
    "score = []\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train,val in kf.split(X, y):\n",
    "    cdn=getCutmoizedDenseNet()\n",
    "    cdn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    cdn.fit(X[train],y[train],epochs=epochs,batch_size=64,verbose=1)\n",
    "    y_train_pred = cdn.predict(X[train],batch_size=32, verbose=1)\n",
    "    threshold=getThreshold(y_train_pred,y[train],0.001)\n",
    "    y_val_pred = 1*(cdn.predict(X[val],batch_size=32, verbose=1)>threshold)    \n",
    "    score.append(f1_score(y[val], y_val_pred, average='micro'))\n",
    "    print(\"F1 Score:\",score[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score:\", np.mean(score),\"+/-\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConvNet():\n",
    "    K.clear_session()\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3),padding='same', input_shape=X.shape[1:],activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3),activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_class))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 14)                7182      \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 894,510\n",
      "Trainable params: 893,486\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cn=getConvNet()\n",
    "cn.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "print(cn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = cn.fit(X,y,epochs=epochs,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16000/16000 [==============================] - 67s - loss: 0.2414    \n",
      "Epoch 2/20\n",
      "16000/16000 [==============================] - 67s - loss: 0.2001    \n",
      "Epoch 3/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.1927    \n",
      "Epoch 4/20\n",
      "16000/16000 [==============================] - 66s - loss: 0.1865    \n",
      "Epoch 5/20\n",
      "16000/16000 [==============================] - 66s - loss: 0.1832    \n",
      "Epoch 6/20\n",
      "16000/16000 [==============================] - 67s - loss: 0.1809    \n",
      "Epoch 7/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.1765    \n",
      "Epoch 8/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.1742    \n",
      "Epoch 9/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.1712    \n",
      "Epoch 10/20\n",
      "16000/16000 [==============================] - 67s - loss: 0.1674    \n",
      "Epoch 11/20\n",
      "16000/16000 [==============================] - 69s - loss: 0.1653    \n",
      "Epoch 12/20\n",
      "16000/16000 [==============================] - 67s - loss: 0.1624    \n",
      "Epoch 13/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.1592    \n",
      "Epoch 14/20\n",
      "16000/16000 [==============================] - 66s - loss: 0.1553    \n",
      "Epoch 15/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.1528    \n",
      "Epoch 16/20\n",
      "16000/16000 [==============================] - 69s - loss: 0.1502    \n",
      "Epoch 17/20\n",
      "16000/16000 [==============================] - 69s - loss: 0.1469    \n",
      "Epoch 18/20\n",
      "16000/16000 [==============================] - 69s - loss: 0.1450    \n",
      "Epoch 19/20\n",
      "16000/16000 [==============================] - 69s - loss: 0.1405    \n",
      "Epoch 20/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.1380    \n",
      "16000/16000 [==============================] - 27s    \n",
      "Best threshold for class 0 ï¼š 0.132\n",
      "Best threshold for class 1 ï¼š 0.121\n",
      "Best threshold for class 2 ï¼š 0.269\n",
      "Best threshold for class 3 ï¼š 0.51\n",
      "Best threshold for class 4 ï¼š 0.218\n",
      "Best threshold for class 5 ï¼š 0.472\n",
      "Best threshold for class 6 ï¼š 0.368\n",
      "Best threshold for class 7 ï¼š 0.519\n",
      "Best threshold for class 8 ï¼š 0.465\n",
      "Best threshold for class 9 ï¼š 0.574\n",
      "Best threshold for class 10 ï¼š 0.453\n",
      "Best threshold for class 11 ï¼š 0.086\n",
      "Best threshold for class 12 ï¼š 0.331\n",
      "Best threshold for class 13 ï¼š 0.267\n",
      "3968/4000 [============================>.] - ETA: 0sF1 Score: 0.432895488292\n",
      "Epoch 1/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.2429    \n",
      "Epoch 2/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.2034    \n",
      "Epoch 3/20\n",
      "16000/16000 [==============================] - 70s - loss: 0.1936    \n",
      "Epoch 4/20\n",
      "16000/16000 [==============================] - 71s - loss: 0.1893    \n",
      "Epoch 5/20\n",
      "16000/16000 [==============================] - 69s - loss: 0.1859    \n",
      "Epoch 6/20\n",
      "16000/16000 [==============================] - 73s - loss: 0.1829    \n",
      "Epoch 7/20\n",
      "16000/16000 [==============================] - 73s - loss: 0.1799    \n",
      "Epoch 8/20\n",
      "16000/16000 [==============================] - 86s - loss: 0.1773    \n",
      "Epoch 9/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1752    \n",
      "Epoch 10/20\n",
      "16000/16000 [==============================] - 92s - loss: 0.1729    \n",
      "Epoch 11/20\n",
      "16000/16000 [==============================] - 87s - loss: 0.1695    \n",
      "Epoch 12/20\n",
      "16000/16000 [==============================] - 88s - loss: 0.1671    \n",
      "Epoch 13/20\n",
      "16000/16000 [==============================] - 92s - loss: 0.1648    \n",
      "Epoch 14/20\n",
      "16000/16000 [==============================] - 93s - loss: 0.1619    \n",
      "Epoch 15/20\n",
      "16000/16000 [==============================] - 90s - loss: 0.1582    \n",
      "Epoch 16/20\n",
      "16000/16000 [==============================] - 68s - loss: 0.1551    \n",
      "Epoch 17/20\n",
      "16000/16000 [==============================] - 65s - loss: 0.1523    \n",
      "Epoch 18/20\n",
      "16000/16000 [==============================] - 71s - loss: 0.1500    \n",
      "Epoch 19/20\n",
      "16000/16000 [==============================] - 74s - loss: 0.1466    \n",
      "Epoch 20/20\n",
      "16000/16000 [==============================] - 67s - loss: 0.1432    \n",
      "15968/16000 [============================>.] - ETA: 0sBest threshold for class 0 ï¼š 0.141\n",
      "Best threshold for class 1 ï¼š 0.167\n",
      "Best threshold for class 2 ï¼š 0.312\n",
      "Best threshold for class 3 ï¼š 0.496\n",
      "Best threshold for class 4 ï¼š 0.188\n",
      "Best threshold for class 5 ï¼š 0.501\n",
      "Best threshold for class 6 ï¼š 0.493\n",
      "Best threshold for class 7 ï¼š 0.411\n",
      "Best threshold for class 8 ï¼š 0.472\n",
      "Best threshold for class 9 ï¼š 0.522\n",
      "Best threshold for class 10 ï¼š 0.506\n",
      "Best threshold for class 11 ï¼š 0.196\n",
      "Best threshold for class 12 ï¼š 0.363\n",
      "Best threshold for class 13 ï¼š 0.241\n",
      "4000/4000 [==============================] - 6s     \n",
      "F1 Score: 0.420769919427\n",
      "Epoch 1/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.2444    \n",
      "Epoch 2/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.2037    \n",
      "Epoch 3/20\n",
      "16000/16000 [==============================] - 89s - loss: 0.1942    \n",
      "Epoch 4/20\n",
      "16000/16000 [==============================] - 92s - loss: 0.1890    \n",
      "Epoch 5/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1849    \n",
      "Epoch 6/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1809    \n",
      "Epoch 7/20\n",
      "16000/16000 [==============================] - 93s - loss: 0.1779    \n",
      "Epoch 8/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1745    \n",
      "Epoch 9/20\n",
      "16000/16000 [==============================] - 88s - loss: 0.1712    \n",
      "Epoch 10/20\n",
      "16000/16000 [==============================] - 89s - loss: 0.1689    \n",
      "Epoch 11/20\n",
      "16000/16000 [==============================] - 89s - loss: 0.1664    \n",
      "Epoch 12/20\n",
      "16000/16000 [==============================] - 87s - loss: 0.1632    \n",
      "Epoch 13/20\n",
      "16000/16000 [==============================] - 87s - loss: 0.1597    \n",
      "Epoch 14/20\n",
      "16000/16000 [==============================] - 88s - loss: 0.1568    \n",
      "Epoch 15/20\n",
      "16000/16000 [==============================] - 90s - loss: 0.1535    \n",
      "Epoch 16/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1505    \n",
      "Epoch 17/20\n",
      "16000/16000 [==============================] - 83s - loss: 0.1482    \n",
      "Epoch 18/20\n",
      "16000/16000 [==============================] - 85s - loss: 0.1449    \n",
      "Epoch 19/20\n",
      "16000/16000 [==============================] - 76s - loss: 0.1412    \n",
      "Epoch 20/20\n",
      "16000/16000 [==============================] - 69s - loss: 0.1390    \n",
      "16000/16000 [==============================] - 20s    \n",
      "Best threshold for class 0 ï¼š 0.225\n",
      "Best threshold for class 1 ï¼š 0.226\n",
      "Best threshold for class 2 ï¼š 0.292\n",
      "Best threshold for class 3 ï¼š 0.452\n",
      "Best threshold for class 4 ï¼š 0.29\n",
      "Best threshold for class 5 ï¼š 0.469\n",
      "Best threshold for class 6 ï¼š 0.316\n",
      "Best threshold for class 7 ï¼š 0.395\n",
      "Best threshold for class 8 ï¼š 0.385\n",
      "Best threshold for class 9 ï¼š 0.556\n",
      "Best threshold for class 10 ï¼š 0.561\n",
      "Best threshold for class 11 ï¼š 0.12\n",
      "Best threshold for class 12 ï¼š 0.131\n",
      "Best threshold for class 13 ï¼š 0.251\n",
      "3968/4000 [============================>.] - ETA: 0sF1 Score: 0.412601315186\n",
      "Epoch 1/20\n",
      "16000/16000 [==============================] - 75s - loss: 0.2470    \n",
      "Epoch 2/20\n",
      "16000/16000 [==============================] - 44s - loss: 0.2041    \n",
      "Epoch 3/20\n",
      "16000/16000 [==============================] - 60s - loss: 0.1952    \n",
      "Epoch 4/20\n",
      "16000/16000 [==============================] - 48s - loss: 0.1896    \n",
      "Epoch 5/20\n",
      "16000/16000 [==============================] - 57s - loss: 0.1855    \n",
      "Epoch 6/20\n",
      "16000/16000 [==============================] - 70s - loss: 0.1822    \n",
      "Epoch 7/20\n",
      "16000/16000 [==============================] - 50s - loss: 0.1799    \n",
      "Epoch 8/20\n",
      "16000/16000 [==============================] - 56s - loss: 0.1769    \n",
      "Epoch 9/20\n",
      "16000/16000 [==============================] - 75s - loss: 0.1737    \n",
      "Epoch 10/20\n",
      "16000/16000 [==============================] - 75s - loss: 0.1727    \n",
      "Epoch 11/20\n",
      "16000/16000 [==============================] - 74s - loss: 0.1693    \n",
      "Epoch 12/20\n",
      "16000/16000 [==============================] - 72s - loss: 0.1666    \n",
      "Epoch 13/20\n",
      "16000/16000 [==============================] - 72s - loss: 0.1646    \n",
      "Epoch 14/20\n",
      "16000/16000 [==============================] - 73s - loss: 0.1609    \n",
      "Epoch 15/20\n",
      "16000/16000 [==============================] - 69s - loss: 0.1583    \n",
      "Epoch 16/20\n",
      "16000/16000 [==============================] - 71s - loss: 0.1548    \n",
      "Epoch 17/20\n",
      "16000/16000 [==============================] - 71s - loss: 0.1519    \n",
      "Epoch 18/20\n",
      "16000/16000 [==============================] - 64s - loss: 0.1481    - ETA\n",
      "Epoch 19/20\n",
      "16000/16000 [==============================] - 63s - loss: 0.1468    \n",
      "Epoch 20/20\n",
      "16000/16000 [==============================] - 73s - loss: 0.1424    \n",
      "16000/16000 [==============================] - 21s    \n",
      "Best threshold for class 0 ï¼š 0.223\n",
      "Best threshold for class 1 ï¼š 0.205\n",
      "Best threshold for class 2 ï¼š 0.337\n",
      "Best threshold for class 3 ï¼š 0.576\n",
      "Best threshold for class 4 ï¼š 0.211\n",
      "Best threshold for class 5 ï¼š 0.491\n",
      "Best threshold for class 6 ï¼š 0.357\n",
      "Best threshold for class 7 ï¼š 0.373\n",
      "Best threshold for class 8 ï¼š 0.354\n",
      "Best threshold for class 9 ï¼š 0.536\n",
      "Best threshold for class 10 ï¼š 0.512\n",
      "Best threshold for class 11 ï¼š 0.126\n",
      "Best threshold for class 12 ï¼š 0.356\n",
      "Best threshold for class 13 ï¼š 0.162\n",
      "3968/4000 [============================>.] - ETA: 0sF1 Score: 0.422706388024\n",
      "Epoch 1/20\n",
      "16000/16000 [==============================] - 64s - loss: 0.2424    \n",
      "Epoch 2/20\n",
      "16000/16000 [==============================] - 73s - loss: 0.2022    \n",
      "Epoch 3/20\n",
      "16000/16000 [==============================] - 88s - loss: 0.1931    \n",
      "Epoch 4/20\n",
      "16000/16000 [==============================] - 81s - loss: 0.1892    \n",
      "Epoch 5/20\n",
      "16000/16000 [==============================] - 90s - loss: 0.1850    \n",
      "Epoch 6/20\n",
      "16000/16000 [==============================] - 90s - loss: 0.1816    \n",
      "Epoch 7/20\n",
      "16000/16000 [==============================] - 94s - loss: 0.1780    \n",
      "Epoch 8/20\n",
      "16000/16000 [==============================] - 90s - loss: 0.1762    \n",
      "Epoch 9/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1738    \n",
      "Epoch 10/20\n",
      "16000/16000 [==============================] - 89s - loss: 0.1701    \n",
      "Epoch 11/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1683    \n",
      "Epoch 12/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1660    \n",
      "Epoch 13/20\n",
      "16000/16000 [==============================] - 93s - loss: 0.1631    \n",
      "Epoch 14/20\n",
      "16000/16000 [==============================] - 91s - loss: 0.1599    \n",
      "Epoch 15/20\n",
      "16000/16000 [==============================] - 92s - loss: 0.1573    \n",
      "Epoch 16/20\n",
      "16000/16000 [==============================] - 94s - loss: 0.1553    \n",
      "Epoch 17/20\n",
      "16000/16000 [==============================] - 90s - loss: 0.1513    \n",
      "Epoch 18/20\n",
      "16000/16000 [==============================] - 92s - loss: 0.1484    \n",
      "Epoch 19/20\n",
      "16000/16000 [==============================] - 92s - loss: 0.1450    \n",
      "Epoch 20/20\n",
      "16000/16000 [==============================] - 98s - loss: 0.1422    \n",
      "16000/16000 [==============================] - 30s    \n",
      "Best threshold for class 0 ï¼š 0.281\n",
      "Best threshold for class 1 ï¼š 0.125\n",
      "Best threshold for class 2 ï¼š 0.211\n",
      "Best threshold for class 3 ï¼š 0.288\n",
      "Best threshold for class 4 ï¼š 0.179\n",
      "Best threshold for class 5 ï¼š 0.403\n",
      "Best threshold for class 6 ï¼š 0.359\n",
      "Best threshold for class 7 ï¼š 0.4\n",
      "Best threshold for class 8 ï¼š 0.439\n",
      "Best threshold for class 9 ï¼š 0.428\n",
      "Best threshold for class 10 ï¼š 0.414\n",
      "Best threshold for class 11 ï¼š 0.122\n",
      "Best threshold for class 12 ï¼š 0.195\n",
      "Best threshold for class 13 ï¼š 0.229\n",
      "3968/4000 [============================>.] - ETA: 0sF1 Score: 0.419277827514\n",
      "CPU times: user 5h 7min 12s, sys: 42min, total: 5h 49min 12s\n",
      "Wall time: 2h 59min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "#5-fold cross validation\n",
    "score = []\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train,val in kf.split(X, y):\n",
    "    cn=getConvNet()\n",
    "    cn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    cn.fit(X[train],y[train],epochs=epochs,batch_size=64,verbose=1)\n",
    "    y_train_pred = cn.predict(X[train],batch_size=32, verbose=1)\n",
    "    threshold=getThreshold(y_train_pred,y[train],0.001)\n",
    "    y_val_pred = 1*(cn.predict(X[val],batch_size=32, verbose=1)>threshold)    \n",
    "    score.append(f1_score(y[val], y_val_pred, average='micro'))\n",
    "    print(\"F1 Score:\",score[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.421650187689 +/- 0.0065705007937\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score:\", np.mean(score),\"+/-\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we just guess the label according to the probability of zero and one of each label......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of random guess: 0.181511437503\n"
     ]
    }
   ],
   "source": [
    "random=np.zeros((N,num_class))\n",
    "for i in range(num_class):\n",
    "    p_0=(y[:,i]==0).sum()/N\n",
    "    random[:,i] = np.random.choice([0,1], N, p=[p_0, 1-p_0])\n",
    "\n",
    "print(\"The score of random guess:\",f1_score(y,random, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your prediction for the testset.  These should be returned as a matrix with one row for each test set image.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is as follows (alphabetic order of the label names):\n",
    "\n",
    "    baby bird car clouds dog female flower male night people portrait river sea tree\n",
    "\n",
    "An example row could like like this if your system predicts the presense of a bird and clouds:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
    "    \n",
    "If you have the matrix prepared in `y` (e.g., by calling `y=model.predict(x_test)`) you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation and Disscussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original DenseNet with 32 filters each layer, 3 dense blocks,2 layers each block,0.2 dropout rate and 12 growth rate\n",
    " 0.232187499321 +/- 0.0173297726876\n",
    " \n",
    " - Original DenseNet : a dead end. Several reason: designed to serve a different purpose: multi-class. not enough time to get fully trained, imbalanced data.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
