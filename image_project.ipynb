{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Wednesday December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to a set of images.  The images are originally from a photo-sharing site and released under Creative Commons-licenses allowing sharing.  The training set contains 20 000 images. We have resized them and cropped them to 128x128 to make the task a bit more manageable.\n",
    "\n",
    "We're only giving you the code for downloading the data. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the image project:\n",
    "\n",
    "- One image may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are images that don't belong to any of our classes, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "\n",
    "- As the dataset is pretty imbalanced, don't focus too strictly on the outputs being probabilistic. (Meaning that the right threshold for selecting the label might not be 0.5.)\n",
    "\n",
    "- Image files can be loaded as numpy matrices for example using `imageio.imread`. Most images are color, but a few grayscale. You need to handle the grayscale ones somehow as they would have a different number of color channels (depth) than the color ones.\n",
    "\n",
    "- Loading all the images into one big matrix as we have done in the exercises is not feasible (e.g. the virtual servers in CSC have only 3 GB of RAM). You need to load the images in smaller chunks for the training. This shouldn't be a problem we are doing mini-batch training anyway, and thus we don't need to keep all the images in memory. You can simply pass you current chunk of images to `model.fit()` as it remembers the weights from the previous run.\n",
    "\n",
    "- You need to think carefully about how you load the annotations and match them up with the corresponding images, especially as you are loading them in smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.helsinki.fi/u/mvsjober/misc/dl2017-image-proj.zip\n",
      "277667840/279523357 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./train/dl2017-image-proj.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "database_path = 'train/'\n",
    "\n",
    "dl_file='dl2017-image-proj.zip'\n",
    "dl_url='https://www.cs.helsinki.fi/u/mvsjober/misc/'\n",
    "get_file(dl_file, dl_url+dl_file, cache_dir='./', cache_subdir=database_path, extract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloaded and extracted the data files into the `train` subdirectory.\n",
    "\n",
    "The images can be found in `train/images`, and are named as `im1.jpg`, `im2.jpg` and so on until `im20000.jpg`.\n",
    "\n",
    "The class labels, or annotations, can be found in `train/annotations` as `CLASSNAME.txt`, where CLASSNAME is one of the fourteen classes: *baby, bird, car, clouds, dog, female, flower, male, night, people, portrait, river, sea,* and *tree*.\n",
    "\n",
    "Each annotation file is a simple text file that lists the images that depict that class, one per line. The images are listed with their number, not the full filename. For example `5969` refers to the image `im5969.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import misc\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = {0:\"baby\",\n",
    "                 1:\"bird\",\n",
    "                 2:\"car\",\n",
    "                 3:\"clouds\",\n",
    "                 4:\"dog\",\n",
    "                 5:\"female\",\n",
    "                 6:\"flower\",\n",
    "                 7:\"male\",\n",
    "                 8:\"night\",\n",
    "                 9:\"people\",\n",
    "                 10:\"portrait\",\n",
    "                 11:\"river\",\n",
    "                 12:\"sea\",\n",
    "                 13:\"tree\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label and file name loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20000\n",
    "num_class = len(class_indices)\n",
    "\n",
    "\n",
    "img_list = [database_path+\"images/\"+\"im\"+str(x)+\".jpg\" for x in range(1,N+1)]\n",
    "y = np.zeros((N,num_class))\n",
    "\n",
    "for k,v in class_indices.items():\n",
    "    with open(database_path+\"annotations/\"+v+\".txt\",\"r\") as fp:\n",
    "        for line in fp:\n",
    "            y[int(line)-1,k] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (32,32)\n",
    "\n",
    "image_load_func = lambda path: misc.imresize(misc.imread(path,mode=\"RGB\"),size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "people\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABrCAYAAABnlHmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3dcb1a2f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example=3\n",
    "img = image_load_func(img_list[example])\n",
    "cmaps = [plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]\n",
    "for l in range(3):\n",
    "    plt.subplot(1, 4, l+1)\n",
    "    plt.imshow(img[:,:,l],cmap=cmaps[l])\n",
    "    plt.axis('off')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "if sum(y[example]==1)==0:\n",
    "    print(\"No label\")\n",
    "else:\n",
    "    for i in np.argwhere(y[example]==1).flatten():\n",
    "        print(class_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack([image_load_func(file) for file in img_list])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from densenet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThreshold(y_pred,y_true,threshold_interval):\n",
    "    threshold = np.arange(0,1,threshold_interval)\n",
    "\n",
    "    best_threshold = np.zeros(y_true.shape[1])\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        temp = np.array([[1 if pred>j else 0 for j in threshold] for pred in y_pred[:,i]])\n",
    "        score = np.array([f1_score(y_true[:,i],temp[:,j], average='micro') for j in range(len(threshold)) ])\n",
    "        best_threshold[i] = threshold[score.argmax()]\n",
    "        print(\"Best threshold for class\",i,\"ï¼š\",best_threshold[i])\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard DenseNet with output layer adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-parameter\n",
    "nb_filter=32\n",
    "nb_dense_block=3\n",
    "nb_layers=3\n",
    "growth_rate=36\n",
    "dropout_rate=0.3\n",
    "weight_decay=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv2D)          (None, 32, 32, 32)    864         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 32, 32, 32)    128         initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32, 32, 32)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 32, 36)    10404       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 32, 32, 36)    0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 32, 32, 68)    0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 32, 32, 68)    128         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32, 32, 68)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 32, 32, 36)    22068       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 32, 32, 36)    0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 32, 32, 104)   0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 32, 32, 104)   128         concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32, 32, 104)   0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 32, 32, 36)    33732       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32, 32, 36)    0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 32, 32, 140)   0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "                                                                   dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 32, 32, 140)   128         concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 32, 32, 140)   0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 32, 32, 140)   19740       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 32, 32, 140)   0           conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 16, 16, 140)   0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 16, 16, 140)   64          average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 16, 16, 140)   0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 16, 16, 36)    45396       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 16, 16, 36)    0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 16, 16, 176)   0           average_pooling2d_1[0][0]        \n",
      "                                                                   dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 16, 16, 176)   64          concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 16, 16, 176)   0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 16, 16, 36)    57060       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 16, 16, 36)    0           conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 16, 16, 212)   0           average_pooling2d_1[0][0]        \n",
      "                                                                   dropout_5[0][0]                  \n",
      "                                                                   dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 16, 16, 212)   64          concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 16, 16, 212)   0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 16, 16, 36)    68724       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 16, 16, 36)    0           conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 16, 16, 248)   0           average_pooling2d_1[0][0]        \n",
      "                                                                   dropout_5[0][0]                  \n",
      "                                                                   dropout_6[0][0]                  \n",
      "                                                                   dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 16, 16, 248)   64          concatenate_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 16, 16, 248)   0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 16, 16, 248)   61752       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 16, 16, 248)   0           conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePool (None, 8, 8, 248)     0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 8, 8, 248)     32          average_pooling2d_2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 8, 8, 248)     0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 8, 8, 36)      80388       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 8, 8, 36)      0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 8, 8, 284)     0           average_pooling2d_2[0][0]        \n",
      "                                                                   dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 8, 8, 284)     32          concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 8, 8, 284)     0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 8, 8, 36)      92052       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 8, 8, 36)      0           conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, 8, 8, 320)     0           average_pooling2d_2[0][0]        \n",
      "                                                                   dropout_9[0][0]                  \n",
      "                                                                   dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 8, 8, 320)     32          concatenate_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 8, 8, 320)     0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 8, 8, 36)      103716      activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 8, 8, 36)      0           conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 8, 8, 356)     0           average_pooling2d_2[0][0]        \n",
      "                                                                   dropout_9[0][0]                  \n",
      "                                                                   dropout_10[0][0]                 \n",
      "                                                                   dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 8, 8, 356)     32          concatenate_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 8, 8, 356)     0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 356)           0           activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 14)            4998        global_average_pooling2d_1[0][0] \n",
      "====================================================================================================\n",
      "Total params: 601,790\n",
      "Trainable params: 601,342\n",
      "Non-trainable params: 448\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dn = DenseNet(num_class, X.shape[1:], nb_layers*3+4, nb_dense_block, growth_rate,\n",
    "             nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "dn.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam')\n",
    "\n",
    "print(dn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(dn, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = dn.fit(X,y,epochs=epochs,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  576/16000 [>.............................] - ETA: 1323s - loss: 0.3296"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "#5-fold cross validation\n",
    "score = []\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train,val in kf.split(X, y):\n",
    "    dn = DenseNet(num_class, X.shape[1:], nb_layers*3+4, nb_dense_block, growth_rate,\n",
    "             nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "    dn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    dn.fit(X[train],y[train],epochs=epochs,batch_size=64,verbose=1)\n",
    "    y_train_pred = dn.predict(X[train],batch_size=32, verbose=1)\n",
    "    threshold=getThreshold(y_train_pred,y[train],0.001)\n",
    "    y_val_pred = 1*(dn.predict(X[val],batch_size=32, verbose=1)>threshold)    \n",
    "    score.append(f1_score(y[val], y_val_pred, average='micro'))\n",
    "print(\"F1 Score:\", np.mean(score),\"+/-\", np.std(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.232187499321 +/- 0.0173297726876\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score:\", np.mean(score),\"+/-\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCutmoizedDenseNet():\n",
    "    nb_filter=32\n",
    "    weight_decay=0.00001\n",
    "    dropout_rate=0.2\n",
    "    nb_dense_block=3\n",
    "    nb_layers=2\n",
    "    growth_rate=1\n",
    "    \n",
    "    model_input = Input(shape=X.shape[1:])\n",
    "\n",
    "    x = Conv2D(nb_filter, (3, 3),\n",
    "                   kernel_initializer=\"he_uniform\",\n",
    "                   padding=\"same\",\n",
    "                   name=\"initial_conv2D\",\n",
    "                   use_bias=True,\n",
    "                   kernel_regularizer=l2(weight_decay))(model_input)\n",
    "    #blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = denseblock(x, nb_layers, nb_filter, growth_rate,\n",
    "                                  dropout_rate=dropout_rate)\n",
    "        # add transition\n",
    "        x = transition(x, nb_filter, dropout_rate=dropout_rate,\n",
    "                       weight_decay=weight_decay)\n",
    "\n",
    "    #last block \n",
    "    x, nb_filter = denseblock(x, nb_layers, nb_filter, growth_rate,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  weight_decay=weight_decay)\n",
    "\n",
    "    x = BatchNormalization(axis=1,\n",
    "                           gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D(data_format=K.image_data_format())(x)\n",
    "    #more layers\n",
    "\n",
    "    model_output = Dense(num_class, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[model_input], outputs=[model_output], name=\"DenseNet\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdn=getCutmoizedDenseNet()\n",
    "cdn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "print(cdn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(cdn, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = cdn.fit(X,y,epochs=epochs,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "#5-fold cross validation\n",
    "score = []\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train,val in kf.split(X, y):\n",
    "    cdn=getCutmoizedDenseNet()\n",
    "    cdn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    cdn.fit(X[train],y[train],epochs=epochs,batch_size=64,verbose=1)\n",
    "    y_train_pred = cdn.predict(X[train],batch_size=32, verbose=1)\n",
    "    threshold=getThreshold(y_train_pred,y[train],0.001)\n",
    "    y_val_pred = 1*(cdn.predict(X[val],batch_size=32, verbose=1)>threshold)    \n",
    "    score.append(f1_score(y[val], y_val_pred, average='micro'))\n",
    "print(\"F1 Score:\", np.mean(score),\"+/-\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConvNet():\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3),padding='same', input_shape=X.shape[1:],activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3),activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_class))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn=getConvNet()\n",
    "cn.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "print(cn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = cn.fit(X,y,epochs=epochs,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "#5-fold cross validation\n",
    "score = []\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train,val in kf.split(X, y):\n",
    "    cn=getConvNet()\n",
    "    cn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    cn.fit(X[train],y[train],epochs=epochs,batch_size=64,verbose=1)\n",
    "    y_train_pred = cn.predict(X[train],batch_size=32, verbose=1)\n",
    "    threshold=getThreshold(y_train_pred,y[train],0.001)\n",
    "    y_val_pred = 1*(cn.predict(X[val],batch_size=32, verbose=1)>threshold)    \n",
    "    score.append(f1_score(y[val], y_val_pred, average='micro'))\n",
    "print(\"F1 Score:\", np.mean(score),\"+/-\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we just guess the label according to the probability of zero and one of each label......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of random guess: 0.181511437503\n"
     ]
    }
   ],
   "source": [
    "random=np.zeros((N,num_class))\n",
    "for i in range(num_class):\n",
    "    p_0=(y[:,i]==0).sum()/N\n",
    "    random[:,i] = np.random.choice([0,1], N, p=[p_0, 1-p_0])\n",
    "\n",
    "print(\"The score of random guess:\",f1_score(y,random, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your prediction for the testset.  These should be returned as a matrix with one row for each test set image.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is as follows (alphabetic order of the label names):\n",
    "\n",
    "    baby bird car clouds dog female flower male night people portrait river sea tree\n",
    "\n",
    "An example row could like like this if your system predicts the presense of a bird and clouds:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
    "    \n",
    "If you have the matrix prepared in `y` (e.g., by calling `y=model.predict(x_test)`) you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
